{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The goal of the MLapp project is to provide the following:\n",
    "\n",
    "1. Illustrate how to build machine learning powered developer tools using the [GitHub Api](https://developer.github.com/v3/) and Flask.  We would like to show data scientists how to build exciting data products using machine learning on the GitHub marketplace, that developers can use.  Specifically, we will build an illustrative data product that will automatically label issues.  \n",
    "\n",
    "2. Gather feedback and iterate \n",
    "\n",
    "\n",
    "The scope of this notebook is to addresses part of goal #1, by illustrating how we can acquire a dataset of GitHub issue labels and train a classifier.  \n",
    "\n",
    "The top issues on GitHub by count are illustrated in [this spreadsheet](https://docs.google.com/spreadsheets/d/1NPacnVsyZMBneeewvPGhCx512A1RPYf8ktDN_RpKeS4/edit?usp=sharing).  To keep things simple, we will build a model to classify an issue as a `bug`, `feature` or `question`.  We use hueristics to collapse a set of issue labels into these three categories, which can be viewed [in this query](https://console.cloud.google.com/bigquery?sq=123474043329:01abf8866144486f932c756730ddaff1).  \n",
    "\n",
    "The heueristic for these class labels are contained within the below case statement:\n",
    "\n",
    "```{sql}\n",
    "  CASE when labels like '%bug%' and labels not like '%not bug%' then True else False end as Bug_Flag,\n",
    "  CASE when labels like '%feature%' or labels like '%enhancement%' or labels like '%improvement%' or labels like '%request%' then True else False end as Feature_Flag,\n",
    "  CASE when labels like '%question%' or labels like '%discussion%' then True else False end as Question_Flag,\n",
    "```\n",
    "    the above case statement is located within [this query](https://console.cloud.google.com/bigquery?sq=123474043329:01abf8866144486f932c756730ddaff1)\n",
    "    \n",
    "\n",
    "The following alternative projects were tried before this task that we did not pursue further:\n",
    " - Transfer learning using the [GitHub Issue Summarizer](https://github.com/hamelsmu/Seq2Seq_Tutorial) to enable the prediction of custom labels on existing repos.  Found that this did not work well as there is a considerable amount of noise with regards to custom labels in repositories and often not enough data to adequately predict this.  \n",
    " - Tried to classify more than the above three classes, however the human-labeled issues are very subjective and it is not clear what is a question vs. a bug.  \n",
    " - Tried multi-label classification since labels can co-occur.  There is very little overlap between `bug`, `feature` and `question` labels, so we decided to simplify things and make this a multi-class classificaiton problem instead.  \n",
    "\n",
    "\n",
    "Note: the code in this notebook was executed on a [p3.8xlarge](https://aws.amazon.com/ec2/instance-types/p3/) instance on AWS.\n",
    "\n",
    "## Outline \n",
    "\n",
    "This notebook will follow these steps:\n",
    "\n",
    "1. Download and partition dataset\n",
    "2. Pre-process dataset\n",
    "2. Build model architecture & Train Model\n",
    "3. Evaluate Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Partition Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 270,624 rows 10 columns\n",
      "Test: 47,758 rows 10 columns\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv(f'https://storage.googleapis.com/codenet/issue_labels/00000000000{i}.csv.gz')\n",
    "                for i in range(1)])\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if row['class_int'] == 2:\n",
    "        df.at[i,'class_int'] = 1\n",
    "\n",
    "#split data into train/test\n",
    "traindf, testdf = train_test_split(df, test_size=.15)\n",
    "\n",
    "traindf.to_pickle('traindf.pkl')\n",
    "testdf.to_pickle('testdf.pkl')\n",
    "\n",
    "#print out stats about shape of data\n",
    "print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
    "print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>repo</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>num_labels</th>\n",
       "      <th>labels</th>\n",
       "      <th>c_bug</th>\n",
       "      <th>c_feature</th>\n",
       "      <th>c_question</th>\n",
       "      <th>class_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118616</th>\n",
       "      <td>\"https://github.com/HypothesisWorks/hypothesis/issues/164\"</td>\n",
       "      <td>HypothesisWorks/hypothesis</td>\n",
       "      <td>better stateful testing</td>\n",
       "      <td>hypothesis’s stateful testing is extremely powerful and if you’re not using it you should be.  but… i’ll be the first to admit it’s a little hard to use. the generic api is fine. it’s quite low level but it’s easy to use. the rule based stuff should be easy to use, but there’s a bit too much boiler plate and bundles of variables are a weird second class citizen.  what i’d like to be able to do is make them just behave like strategies, where the strategy’s evaluation is deferred until execution time, so you can use them as if they were any other strategy and everything should just work.  i would also like the syntax for using it to be more unified with the normal given syntax. ideally it would be nice if every given invocation had an implicit state machine associated with it so as to unify the two approaches.</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"enhancement\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53683</th>\n",
       "      <td>\"https://github.com/Nick-Lucas/EntryPoint/issues/29\"</td>\n",
       "      <td>Nick-Lucas/EntryPoint</td>\n",
       "      <td>commands method.invoke will destroy an exception's stacktrace on re-throw</td>\n",
       "      <td>\\r fix may be here here:\\r http://stackoverflow.com/questions/57383/in-c-how-can-i-rethrow-innerexception-without-losing-stack-trace</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"bug\", \"bug\"]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173911</th>\n",
       "      <td>\"https://github.com/spinnaker/spinnaker/issues/2978\"</td>\n",
       "      <td>spinnaker/spinnaker</td>\n",
       "      <td>orca/clouddriver triggers multiple  createservergroup  actions though only one was specified in the pipeline definition</td>\n",
       "      <td>issue summary:\\r \\r we’re sometimes seeing pipelines fail after 10 sec due to two clouddriver threads both attempting the exact same atomic k8s operation, one of which fails and blows up the pipeline . \\r \\r the specific error that's raised is that  create replica set &lt;application-vxx&gt; in default for account brisket failed: replicasets.extensions \\ &lt;application-vxx&gt;\\  already exists . this replica set never exists before the pipeline execution - it is created as a side effect of this pipeline, which promptly fails thereafter citing this error. \\r \\r on investigating further, we saw in the actual pipeline that kato was attempting to run two  createservergroup  pipeline stages from the same  deploy  stage parent, even though the pipeline configuration only specifies one. this feels like a bug in spinnaker 1.7.5.\\r \\r rerunning the pipeline eventually makes the problem disappear. when successful, our pipelines only have one  createservergroup  stage. \\r \\r this has been affecting all ...</td>\n",
       "      <td>5</td>\n",
       "      <td>[\"bug\", \"component/orca\", \"provider/kubernetes-v1\", \"stale\", \"to-be-closed\"]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               url  \\\n",
       "118616  \"https://github.com/HypothesisWorks/hypothesis/issues/164\"   \n",
       "53683         \"https://github.com/Nick-Lucas/EntryPoint/issues/29\"   \n",
       "173911        \"https://github.com/spinnaker/spinnaker/issues/2978\"   \n",
       "\n",
       "                              repo  \\\n",
       "118616  HypothesisWorks/hypothesis   \n",
       "53683        Nick-Lucas/EntryPoint   \n",
       "173911         spinnaker/spinnaker   \n",
       "\n",
       "                                                                                                                          title  \\\n",
       "118616                                                                                                  better stateful testing   \n",
       "53683                                                 commands method.invoke will destroy an exception's stacktrace on re-throw   \n",
       "173911  orca/clouddriver triggers multiple  createservergroup  actions though only one was specified in the pipeline definition   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           body  \\\n",
       "118616                                                                                                                                                                                      hypothesis’s stateful testing is extremely powerful and if you’re not using it you should be.  but… i’ll be the first to admit it’s a little hard to use. the generic api is fine. it’s quite low level but it’s easy to use. the rule based stuff should be easy to use, but there’s a bit too much boiler plate and bundles of variables are a weird second class citizen.  what i’d like to be able to do is make them just behave like strategies, where the strategy’s evaluation is deferred until execution time, so you can use them as if they were any other strategy and everything should just work.  i would also like the syntax for using it to be more unified with the normal given syntax. ideally it would be nice if every given invocation had an implicit state machine associated with it so as to unify the two approaches.   \n",
       "53683                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \\r fix may be here here:\\r http://stackoverflow.com/questions/57383/in-c-how-can-i-rethrow-innerexception-without-losing-stack-trace   \n",
       "173911  issue summary:\\r \\r we’re sometimes seeing pipelines fail after 10 sec due to two clouddriver threads both attempting the exact same atomic k8s operation, one of which fails and blows up the pipeline . \\r \\r the specific error that's raised is that  create replica set <application-vxx> in default for account brisket failed: replicasets.extensions \\ <application-vxx>\\  already exists . this replica set never exists before the pipeline execution - it is created as a side effect of this pipeline, which promptly fails thereafter citing this error. \\r \\r on investigating further, we saw in the actual pipeline that kato was attempting to run two  createservergroup  pipeline stages from the same  deploy  stage parent, even though the pipeline configuration only specifies one. this feels like a bug in spinnaker 1.7.5.\\r \\r rerunning the pipeline eventually makes the problem disappear. when successful, our pipelines only have one  createservergroup  stage. \\r \\r this has been affecting all ...   \n",
       "\n",
       "        num_labels  \\\n",
       "118616           1   \n",
       "53683            1   \n",
       "173911           5   \n",
       "\n",
       "                                                                              labels  \\\n",
       "118616                                                               [\"enhancement\"]   \n",
       "53683                                                                 [\"bug\", \"bug\"]   \n",
       "173911  [\"bug\", \"component/orca\", \"provider/kubernetes-v1\", \"stale\", \"to-be-closed\"]   \n",
       "\n",
       "        c_bug  c_feature  c_question  class_int  \n",
       "118616  False       True       False          1  \n",
       "53683    True      False       False          0  \n",
       "173911   True      False       False          0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview data\n",
    "traindf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the data:  \n",
    "\n",
    "- url:        url where you can find this issue\n",
    "- repo:       owner/repo name\n",
    "- title:      title of the issue\n",
    "- body:       body of the issue, not including comments\n",
    "- num_labels: number of issue labels\n",
    "- labels:     an array of labels applied a user manually applied to the issue (represented as a string)\n",
    "- c_bug:      boolean flag that indicates if the issue label corresponds to a bug\n",
    "- c_feature:  boolean flag that indicates if the issue label corresponds to a feature\n",
    "- c_question: boolean flag that indicates if the issue label corresponds to a question\n",
    "- class_int:  integer between 0 - 2 that corresponds to the class label.  **0=bug, 1=feature, 2=question**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class frequency **0=bug, 1=feature, 2=question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_int\n",
      "0    121221\n",
      "1    149403\n",
      "dtype: int64\n",
      "class_int\n",
      "0    21295\n",
      "1    26463\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(traindf.groupby('class_int').size())\n",
    "print(testdf.groupby('class_int').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of unique repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg # of issues per repo: 2.6\n",
      " Avg # of issues per org: 2.8\n"
     ]
    }
   ],
   "source": [
    "print(f' Avg # of issues per repo: {len(traindf) / traindf.repo.nunique():.1f}')\n",
    "print(f\" Avg # of issues per org: {len(traindf) / traindf.repo.apply(lambda x: x.split('/')[-1]).nunique():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most popular repos by # of issues:\n",
    "\n",
    " - `pcnt` = percent of total issues in the dataset\n",
    " - `count` = number of issues in the dataset for that repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pcnt</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>repo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Microsoft/vscode</th>\n",
       "      <td>0.005145</td>\n",
       "      <td>1638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rancher/rancher</th>\n",
       "      <td>0.002349</td>\n",
       "      <td>748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MicrosoftDocs/azure-docs</th>\n",
       "      <td>0.002060</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>godotengine/godot</th>\n",
       "      <td>0.001894</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ansible/ansible</th>\n",
       "      <td>0.001866</td>\n",
       "      <td>594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hashicorp/terraform</th>\n",
       "      <td>0.001624</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kubernetes/kubernetes</th>\n",
       "      <td>0.001504</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lionheart/openradar-mirror</th>\n",
       "      <td>0.001432</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dart-lang/sdk</th>\n",
       "      <td>0.001159</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elastic/kibana</th>\n",
       "      <td>0.001156</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eclipse/che</th>\n",
       "      <td>0.001150</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dotnet/corefx</th>\n",
       "      <td>0.001146</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>magento/magento2</th>\n",
       "      <td>0.001040</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brave/browser-laptop</th>\n",
       "      <td>0.001027</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kbower/tickettest1</th>\n",
       "      <td>0.000974</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kademi/kademi-dev</th>\n",
       "      <td>0.000832</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eslint/eslint</th>\n",
       "      <td>0.000801</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>owncloud/core</th>\n",
       "      <td>0.000782</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openshift/origin</th>\n",
       "      <td>0.000773</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elastic/elasticsearch</th>\n",
       "      <td>0.000766</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                pcnt  count\n",
       "repo                                       \n",
       "Microsoft/vscode            0.005145   1638\n",
       "rancher/rancher             0.002349    748\n",
       "MicrosoftDocs/azure-docs    0.002060    656\n",
       "godotengine/godot           0.001894    603\n",
       "ansible/ansible             0.001866    594\n",
       "hashicorp/terraform         0.001624    517\n",
       "kubernetes/kubernetes       0.001504    479\n",
       "lionheart/openradar-mirror  0.001432    456\n",
       "dart-lang/sdk               0.001159    369\n",
       "elastic/kibana              0.001156    368\n",
       "eclipse/che                 0.001150    366\n",
       "dotnet/corefx               0.001146    365\n",
       "magento/magento2            0.001040    331\n",
       "brave/browser-laptop        0.001027    327\n",
       "kbower/tickettest1          0.000974    310\n",
       "Kademi/kademi-dev           0.000832    265\n",
       "eslint/eslint               0.000801    255\n",
       "owncloud/core               0.000782    249\n",
       "openshift/origin            0.000773    246\n",
       "elastic/elasticsearch       0.000766    244"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pareto_df = pd.DataFrame({'pcnt': df.groupby('repo').size() / len(df), 'count': df.groupby('repo').size()})\n",
    "pareto_df.sort_values('pcnt', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process the raw text data, we will use [ktext](https://github.com/hamelsmu/ktext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from ktext.preprocess import processor\n",
    "import dill as dpickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean, tokenize, and apply padding / truncating such that each document length = 75th percentile for the dataset.\n",
    "Retain only the top keep_n words in the vocabulary and set the remaining words to 1 which will become common index for rare words.\n",
    "\n",
    "**Warning:** the below block of code can take a long time to execute.\n",
    "\n",
    "#### Learn the vocabulary from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 105 based upon heuristic of 0.75 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 139 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 9 sec\n",
      "WARNING:root:Finished parsing 270,624 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 6 sec\n",
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 10 based upon heuristic of 0.75 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 14 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 1 sec\n",
      "WARNING:root:Finished parsing 270,624 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 1 sec\n"
     ]
    }
   ],
   "source": [
    "train_body_raw = traindf.body.tolist()\n",
    "train_title_raw = traindf.title.tolist()\n",
    "\n",
    "# Clean, tokenize, and apply padding / truncating such that each document length = 75th percentile for the dataset.\n",
    "#  also, retain only the top keep_n words in the vocabulary and set the remaining words\n",
    "#  to 1 which will become common index for rare words \n",
    "\n",
    "# process the issue body data\n",
    "body_pp = processor(heuristic_pct_padding=.75, keep_n=8000)\n",
    "train_body_vecs = body_pp.fit_transform(train_body_raw)\n",
    "\n",
    "# process the title data\n",
    "title_pp = processor(heuristic_pct_padding=.75, keep_n=4500)\n",
    "train_title_vecs = title_pp.fit_transform(train_title_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply transformations to Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:...tokenizing data\n",
      "WARNING:root:...indexing data\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:...tokenizing data\n",
      "WARNING:root:...indexing data\n",
      "WARNING:root:...padding data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_body_raw = testdf.body.tolist()\n",
    "test_title_raw = testdf.title.tolist()\n",
    "\n",
    "test_body_vecs = body_pp.transform_parallel(test_body_raw)\n",
    "test_title_vecs = title_pp.transform_parallel(test_title_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Labels\n",
    "\n",
    "Add an additional dimension to the end to facilitate compatibility with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.expand_dims(traindf.class_int.values, -1)\n",
    "test_labels = np.expand_dims(testdf.class_int.values, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of rows in data for the body, title and labels should be the same for both train and test partitions\n",
    "assert train_body_vecs.shape[0] == train_title_vecs.shape[0] == train_labels.shape[0]\n",
    "assert test_body_vecs.shape[0] == test_title_vecs.shape[0] == test_labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save pre-processors and data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessor\n",
    "with open('body_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(body_pp, f)\n",
    "\n",
    "with open('title_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(title_pp, f)\n",
    "\n",
    "# Save the processed data\n",
    "np.save('train_title_vecs.npy', train_title_vecs)\n",
    "np.save('train_body_vecs.npy', train_body_vecs)\n",
    "np.save('test_body_vecs.npy', test_body_vecs)\n",
    "np.save('test_title_vecs.npy', test_title_vecs)\n",
    "np.save('train_labels.npy', train_labels)\n",
    "np.save('test_labels.npy', test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Architecture & Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Embedding, BatchNormalization, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import dill as dpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and shape information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('title_pp.dpkl', 'rb') as f:\n",
    "    title_pp = dpickle.load(f)\n",
    "\n",
    "with open('body_pp.dpkl', 'rb') as f:\n",
    "    body_pp = dpickle.load(f)\n",
    "    \n",
    "#load the training data and labels\n",
    "train_body_vecs = np.load('train_body_vecs.npy')\n",
    "train_title_vecs = np.load('train_title_vecs.npy')\n",
    "train_labels = np.load('train_labels.npy')\n",
    "\n",
    "#load the test data and labels\n",
    "test_body_vecs = np.load('test_body_vecs.npy')\n",
    "test_title_vecs = np.load('test_title_vecs.npy')\n",
    "test_labels = np.load('test_labels.npy')\n",
    "\n",
    "\n",
    "issue_body_doc_length = train_body_vecs.shape[1]\n",
    "issue_title_doc_length = train_title_vecs.shape[1]\n",
    "\n",
    "body_vocab_size = body_pp.n_tokens + 1\n",
    "title_vocab_size = title_pp.n_tokens + 1\n",
    "\n",
    "num_classes = len(set(train_labels[:, 0]))\n",
    "assert num_classes == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model Architecture\n",
    "\n",
    "We did very little hyperparameter tuning.  Keeping model simple as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_input = Input(shape=(issue_body_doc_length,), name='Body-Input')\n",
    "title_input = Input(shape=(issue_title_doc_length,), name='Title-Input')\n",
    "\n",
    "body = Embedding(body_vocab_size, 50, name='Body-Embedding')(body_input)\n",
    "title = Embedding(title_vocab_size, 50, name='Title-Embedding')(title_input)\n",
    "\n",
    "body = BatchNormalization()(body)\n",
    "body = GRU(100, name='Body-Encoder')(body)\n",
    "\n",
    "title = BatchNormalization()(title)\n",
    "title = GRU(75, name='Title-Encoder')(title)\n",
    "\n",
    "x = Concatenate(name='Concat')([body, title])\n",
    "x = BatchNormalization()(x)\n",
    "out = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model([body_input, title_input], out)\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001), \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Body-Input (InputLayer)         (None, 105)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Title-Input (InputLayer)        (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Body-Embedding (Embedding)      (None, 105, 50)      400100      Body-Input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Title-Embedding (Embedding)     (None, 10, 50)       225100      Title-Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 105, 50)      200         Body-Embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 10, 50)       200         Title-Embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Body-Encoder (GRU)              (None, 100)          45300       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Title-Encoder (GRU)             (None, 75)           28350       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Concat (Concatenate)            (None, 175)          0           Body-Encoder[0][0]               \n",
      "                                                                 Title-Encoder[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 175)          700         Concat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            352         batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 700,302\n",
      "Trainable params: 699,752\n",
      "Non-trainable params: 550\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "script_name_base = 'Issue_Label_v1'\n",
    "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "model_checkpoint = ModelCheckpoint('{:}_best_model.hdf5'.format(script_name_base),\n",
    "                                   save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valmir/uni/Issue-Label-Bot/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 270624 samples, validate on 47758 samples\n",
      "Epoch 1/4\n",
      "270624/270624 [==============================] - 234s 864us/step - loss: 0.4353 - acc: 0.8040 - val_loss: 0.5185 - val_acc: 0.7525\n",
      "Epoch 2/4\n",
      "270624/270624 [==============================] - 229s 846us/step - loss: 0.3434 - acc: 0.8551 - val_loss: 0.4022 - val_acc: 0.8234\n",
      "Epoch 3/4\n",
      "270624/270624 [==============================] - 229s 846us/step - loss: 0.3085 - acc: 0.8714 - val_loss: 0.3892 - val_acc: 0.8349\n",
      "Epoch 4/4\n",
      "270624/270624 [==============================] - 229s 845us/step - loss: 0.2678 - acc: 0.8904 - val_loss: 0.4171 - val_acc: 0.8304\n"
     ]
    }
   ],
   "source": [
    "batch_size = 900\n",
    "epochs = 4\n",
    "history = model.fit(x=[train_body_vecs, train_title_vecs], \n",
    "                   y=train_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=[(test_body_vecs, test_title_vecs), test_labels], \n",
    "                    callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model\n",
    "\n",
    "Compute a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17593  3702]\n",
      " [ 4183 22280]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "best_model = load_model('Issue_Label_v1_best_model.hdf5')\n",
    "\n",
    "y_pred = np.argmax(best_model.predict(x=[test_body_vecs, test_title_vecs],\n",
    "                                      batch_size=15000),\n",
    "                   axis=1)\n",
    "\n",
    "# get labels\n",
    "y_test = test_labels[:, 0]\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import dill as dpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valmir/uni/Issue-Label-Bot/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#load the best model\n",
    "best_model = load_model('Issue_Label_v1_best_model.hdf5')\n",
    "\n",
    "#load the pre-processors\n",
    "with open('title_pp.dpkl', 'rb') as f:\n",
    "    title_pp = dpickle.load(f)\n",
    "\n",
    "with open('body_pp.dpkl', 'rb') as f:\n",
    "    body_pp = dpickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePrediction(title, body):\n",
    "    titleVec = title_pp.transform([body])\n",
    "    bodyVec = body_pp.transform([title])\n",
    "    class_names=['bug', 'feature_request/question']\n",
    "    probs = best_model.predict(x=[bodyVec, titleVec]).tolist()[0]\n",
    "    print({k:v for k,v in zip(class_names, probs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bug': 0.7058060169219971, 'feature_request/question': 0.2941940128803253}\n"
     ]
    }
   ],
   "source": [
    "makePrediction('maybe error', 'I am not sure if it is my fault but it does not work. Am I missing something?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bug': 0.9252945780754089, 'feature_request/question': 0.0747053399682045}\n"
     ]
    }
   ],
   "source": [
    "makePrediction('nothing works', 'It does` not work, I get bad errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bug': 0.012789001688361168, 'feature_request/question': 0.9872109889984131}\n"
     ]
    }
   ],
   "source": [
    "makePrediction('Add new button', 'Please add a new button')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makePrediction('Help me pleas', 'Please add a new button')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
